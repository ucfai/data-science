{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nwant to work on this locally?\n\n\n1. make conda environment, \n                                    conda create --name workshop\n                                    conda activate workshop\n\n\n\n2. install dependencies wiithin conda-env\n                                    pip install kaggle\n                                    conda install folium \n                                    conda install seaborn\n                                    pip install descartes\n                                    kaggle datasets download -d bbissey/barcelonaairbnbgeojson\n                                    or just download the dataset on kaggle's website\n\n\n^^ maybe some more package installs \n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading In and Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# scraped on sept19, 2019 from insideairbnb.com\ndf_0 = pd.read_csv('../input/barcelonaairbnbgeojson/listings.csv')\nnum_rows = len(df_0['id'])\ndf_0.head(5)\n\n# trouble with only displaying some columns or rows?\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`df.describe()` displays a quick summary of the data with some statistics per column"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"display(df_0.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for Missing / Null Values\n`df.isnull()` returns a table of booleans. Use `sum()` to get the total number of `null` values."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_0.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we want to remove the columns that have greater than 60% of values missing."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove Majority Null Columns\n\ncolsToDrop = []\nfor col in df_0.columns:\n    if df_0[col].isnull().sum() > (.6 * num_rows):\n        colsToDrop.append(col)  \nprint(f'Number of columns to be dropped: {len(colsToDrop)}')\nfor col in colsToDrop:\n    print(col)\ndf_0.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_1 = df_0.drop(colsToDrop, axis=1)\n\ndf_1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And remove the unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Columns with One Unique Value\n# Why? For example, country. If all data is in the same country, we don't need 20404 rows that say Spain\n# Yes, there are faster and more efficient ways to do this.\n\ncolsToDrop = []\n\nfor col in df_1.columns:\n    if df_1[col].nunique() == 1:\n        colsToDrop.append(col)\n        \nfor col in colsToDrop:\n    print(col)\ndf_1.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reassign to df_2 variable\n\ndf_2 = df_1.drop(colsToDrop, axis = 1)\ndf_2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking Data Types\nOur data includes many Strings, which are very useful for any categorical analysis. We will be removing some columns as we don't need."},{"metadata":{"trusted":true},"cell_type":"code","source":"# what data types make up our dataframe?\n# object == string\n\ndf_2.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As previously seen, pandas' nunique function counts the number of unique values in a column\ndisplay(df_2.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bulk Removal of Redundant Columns and String Columns\n\n# Some of these string columns may be helpful for categorical analysis and language processing, \n# but for the purpose of this workshop we will leave them out.\n# \n\n\ndf_3 = df_2.drop(['listing_url', 'last_scraped', 'name', 'summary', 'space', 'description',\n                 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules', 'picture_url',\n                 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_thumbnail_url', \n                  'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_response_time', \n                  'host_response_rate', 'street', 'host_verifications', 'host_picture_url', 'amenities', 'calendar_updated', \n                  'calendar_last_scraped', 'availability_30', 'availability_90', 'availability_60','neighbourhood', \n                  'smart_location', 'is_location_exact', 'first_review', 'last_review', 'license','minimum_minimum_nights',\n                  'maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm',\n                  'maximum_nights_avg_ntm','calculated_host_listings_count_entire_homes',\n                  'calculated_host_listings_count_shared_rooms','calculated_host_listings_count_private_rooms'], axis =1)\nprint(f'Before removing redundant values: {df_2.shape}')\nprint(f'After removing redundant values:  {df_3.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at what data remains, along with how many unique values there are\ndf_3.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you may have noticed, everytime we display the data the ID has been its own column. Set that as the index."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3 = df_3.set_index('id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Proportions of Data\nLooking at how the data is distributed can help when working through a dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Why? To visualize the proportions of data relative to each other\n# Normalization of value counts helps with seeing proportions easier\n\ndf_3.room_type.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3.cancellation_policy.value_counts(normalize=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3.neighbourhood_group_cleansed.value_counts(normalize=True)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3.bed_type.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Almost everybody has a real bed, let's remove this variable\ndf_4 = df_3.drop(['bed_type', 'review_scores_accuracy',\n'review_scores_cleanliness',\n'review_scores_checkin',\n'review_scores_communication',\n'review_scores_location',\n'review_scores_value',\n'state',\n'zipcode',\n'market',\n], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check nulls again\n\ndf_4.isnull().sum()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When `availability_365` is 0, the listing is not active. We take those out too."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns every row where the 'availability_365' column equals 0.\ndf_4.loc[df_4['availability_365'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removal of Inactive Listings\n\ndf_4 = df_4[df_4['availability_365'] != 0] \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting strings for price into floats"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parsing Floats from Price Columns\n\ndf_5 = df_4\ndf_6 = df_5\ndf_6['price'] = df_6['price'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['security_deposit'] = df_6['security_deposit'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['cleaning_fee'] = df_6['cleaning_fee'].str.replace('$', '').str.replace(',', '').astype(float)\ndf_6['extra_people'] = df_6['extra_people'].str.replace('$', '').str.replace(',', '').astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_6['security_deposit'].fillna(0, inplace = True)\ndf_6['cleaning_fee'].fillna(0, inplace = True)\ndf_6['extra_people'].fillna(0, inplace = True)\ndf_6['review_scores_rating'].fillna(0, inplace = True)\ndf_6['reviews_per_month'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review the remaining data\ndf_6.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping more columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_7 = df_6.drop(['review_scores_rating', 'number_of_reviews', 'number_of_reviews_ltm'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_7.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PRICE LIMIT SETTING AND PRICE DISTRIBUTION\ndf_7.price.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\ndf_7[df_7.price > 1000].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_8 = df_7.drop(df_7[df_7.price > 650].index, axis=0)\ndf_8 = df_8.drop('maximum_nights', axis=1)\ndf_8.price.describe()\n\n#df_7 has prices above 500, df_8 does not. there are about 500 listings priced above 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.violinplot(x=df_8.price, palette = \"Set3\").set_title(\"Price Distribution (removed listings $650 and above)\", size=16)\nsns.despine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_8.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_8.isnull().sum()\n#because there are so few, we will investigate the cases where there is null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cases = df_8[df_8.isnull().any(axis=1)]\nnull_cases.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cases.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping all nulls for `df_9`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9 = df_8.dropna()\ndf_9.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seaborn - Violin Plot\nFirst of the plots, we have the violin plot. It displays the density of data. \nThe distribution is display, along with a boxplot found in the middle."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explain the parts of the violin chart:\n# Curve = Normal Disribution\n# Bottom of veritcal line = Smallest value\n# Top of vertical line = Highest value\n# Bottom of the middle box = First quartile\n# Top of the middle box = Third quartile\n# White dot = mean\n\n\nplt.figure(figsize=(20,10))\nsorted_room = df_9.groupby(['room_type'])['price'].median().sort_values()\nsns.violinplot(x=df_9.room_type, y=df_9.price, order=list(sorted_room.index)).set_title(\"Price by Room Type\", size=16)\n#sns.despine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(20,10))\n\nsorted_room = df_9.groupby(['room_type'])['reviews_per_month'].median().sort_values()\nsns.violinplot(x=df_9.room_type, y=df_9.reviews_per_month, order=list(sorted_room.index)).set_title(\"Reviews Per Month by Room Type\", size=16)\nsns.despine","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distributions will (most likely) not be perfectly symmetric, so we want to look at the skew of each column of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explain skew\ndf_9.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9['minimum_nights'][df_9['minimum_nights'] > 365] = 366","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With `distplot` we can view our distributions to visualize skew."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predictor variables and Skewed Distribution\n\n\nf, axes = plt.subplots(2, 4, figsize=(15, 15))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.price, ax = axes[0,0])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.minimum_nights, ax = axes[1,0])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.calculated_host_listings_count, ax = axes[0,1])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.accommodates, ax = axes[1,1])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.reviews_per_month, ax = axes[0,2])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.bedrooms, ax = axes[0,3])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.bathrooms, ax = axes[1,3])\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_9.extra_people, ax = axes[1,2])\n\n\ndf_9.skew(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.nunique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.minimum_nights.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_9.calculated_host_listings_count.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`bedrooms`, `beds`, `accommodates` are very similar, better to limit to one column.\nAlso limiting the number of unique values on `minimum_nights` by grouping together."},{"metadata":{"trusted":true},"cell_type":"code","source":"# BINNING VARIABLES (drop beds because we have bedrooms)\n\ndf_10 = df_9.drop(['beds', 'accommodates'], axis=1)\ndf_10['binned_min_nights'] = pd.cut(df_9['minimum_nights'], bins=[0, 1, 2, 10, 1900],\n                                                labels = ['oneNight', '2to3','4to10', '11+']\n                                                )\ndf_10.binned_min_nights.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_10.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf_10.calculated_host_listings_count.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to have an approximately normal distribution, we can try to apply a log transformation to our skewed data."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#log transforming price, sec deposit, cleaning fee, extra people, reviews_per_month variables\n\ndf_10['log_price'] = np.log(df_10['price']*10 + 1)\ndf_10['log_security'] = np.log(df_10['security_deposit'] + 1)\ndf_10['log_cleaning'] = np.log(df_10['cleaning_fee']*10 + 1)\n\ndf_10['log_extra_people'] = np.log(df_10['extra_people']*10 + 1)\ndf_10['log_reviews_pm'] = np.log((df_10['reviews_per_month']+ 1) * 10)\ndf_10['log_bedrooms'] = np.log(df_10['bedrooms'] + 1)\ndf_10['log_bathrooms'] = np.log(df_10['bathrooms']*10 + 1)\ndf_10['log_guests_included'] = np.log(df_10['guests_included']*100 + 1)\ndf_10['log_listings_count'] = np.log(df_10['calculated_host_listings_count']*10 + 1)\n\ndf_10.skew()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Through these two distplots, we can see how the log function effected our distribution and skew."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictor variables and Skewed Distribution\n\nsns.set({'xtick.labelsize': 10, 'ytick.labelsize': 10})\n\nf, axes = plt.subplots(2,1, figsize=(5, 10))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.reviews_per_month, ax = axes[0]).set_title(\"Log transformed Reviews Distribution\")\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.log_reviews_pm, ax = axes[1])\n\n\n\ndf_10.skew(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictor variables and Skewed Distribution\n\n\nf, axes = plt.subplots(2,1, figsize=(18, 9))\nsns.despine(left=True)\n\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.price, ax = axes[0]).set_title(\"Log transformed Price Distribution\")\nsns.distplot(color = sns.color_palette(\"Set3\")[0], a=df_10.log_price, ax = axes[1])\n\n\n\ndf_10.skew(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_10.room_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boxplots\nA part of the violin plot. Although we cannot see our distribution in this graph, it is helpful in visualizing outliers and where the majority of data lies."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20))\nsns.set({'xtick.labelsize': 20, 'ytick.labelsize': 20})\nsorted_hood = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\nsns.boxplot(palette = 'Set3', y=df_10.neighbourhood_group_cleansed, x=df_10.log_price, order=list(sorted_hood.index)).set_title(\"Log(Price) by Neighbourhood\", size=20)\nsns.despine","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heatmaps\nThe denser the color, the higher the correlation. A red hue means a positive correlation, a blue hue means a negative correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df_10.corr(), annot=True, cmap='coolwarm', linewidth=.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing more distributions after log"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 2, figsize=(10, 10))\nsns.set()\n\nsns.distplot(df_10.bedrooms, ax = axes[0,0])\nsns.distplot(df_10.log_bedrooms, ax = axes[1,0])\nsns.distplot(df_10.log_reviews_pm, ax = axes[1,1])\nsns.distplot(df_10.reviews_per_month, ax = axes[0,1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# descartes and folium\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import descartes\nimport geopandas as gpd\n\ngjsonFile = \"../input/barcelonaairbnbgeojson/neighbourhoods.geojson\"\nbarc_hoods = gpd.read_file(gjsonFile)\n\nbarc_hoods.plot(figsize=(10,10), column=\"neighbourhood_group\", cmap = \"tab10\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"barc_hoods['neighbourhood_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"barc_hoods.plot(figsize=(20,20), column=\"neighbourhood_group\", cmap='tab10', alpha = .5)\n#plt.figure(figsize=(20,20))\n\nsns.scatterplot(x='longitude', \n                y = 'latitude', \n                hue='price', \n                size = 'price', \n                sizes= (20, 600),\n                alpha = .8,\n                marker=\".\",\n                data = df_10,\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"barc_hoods.plot(figsize=(10,10), alpha = .5)\n#plt.figure(figsize=(20,20))\n#here we add lat and longitude lines to plot for context\n\n\nsns.set({'font.size' : 10})\nsns.set_style(\"whitegrid\")\n#plot by reviews per month\nsns.scatterplot(x='longitude', \n                y = 'latitude', \n                hue='neighbourhood_group_cleansed', \n                alpha = .5,\n                marker=\"o\",\n                data = df_10,\n                cmap='Set3')\n                #size = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nfrom folium.plugins import HeatMap, MarkerCluster\n\nmapp = folium.Map(location=[41.40,2.15], zoom_start=12, figsize=(20,20))\ncluster = MarkerCluster().add_to(mapp)\n# add a marker for every record in the filtered data, use a clustered view\nfor each in df_10.iterrows():\n    folium.Marker(\n        location = [each[1]['latitude'],each[1]['longitude']], \n        clustered_marker = True).add_to(cluster)\n  \nmapp.save(outfile='map.html')\ndisplay(mapp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmax_price_map = df_10['price'].max() #this should be 650\nbarc_map = folium.Map(location=[41.40, 2.15], zoom_start=12, )\n\nheatmap = HeatMap( list(zip(df_10.latitude, df_10.longitude, df_10.price)),\n                 min_opacity = .3,\n                 max_val = max_price_map, \n                 radius = 3,\n                 blur = 2,\n                 max_zoom=1)\n\nfolium.GeoJson(barc_hoods).add_to(barc_map)\nbarc_map.add_child(heatmap)\n\nbarc_map.save(outfile=\"mapp.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport folium\nfrom folium.plugins import HeatMap\n\n\nmax_reviews_pm = df_10['reviews_per_month'].max() \nbarc_map = folium.Map(location=[41.40, 2.15], zoom_start=12, )\n\nheatmap = HeatMap( list(zip(df_10.latitude, df_10.longitude, df_10.reviews_per_month)),\n                 min_opacity = .3,\n                 max_val = max_reviews_pm, \n                 radius = 3,\n                 blur = 2,\n                 max_zoom=1)\n\nfolium.GeoJson(barc_hoods).add_to(barc_map)\nbarc_map.add_child(heatmap)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using boxplot to view relationships between price and other columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\n\nsns.boxplot(x=\"neighbourhood_group_cleansed\",\n            y=\"log_price\",\n            data=df_10, \n            palette=\"tab10\", \n            order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(Price) by neighbourhood\", size=14)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['neighbourhood_group_cleansed'])['reviews_per_month'].median().sort_values()\n\nsns.boxplot(x=\"neighbourhood_group_cleansed\",y=\"log_reviews_pm\",\n            data=df_10, palette=\"tab10\", order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(ReviewsPerMonth) by neighbourhood\", size=14)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(20,10))\nsorted_hood_by_price = df_10.groupby(['room_type'])['log_price'].median().sort_values()\n\nsns.boxplot(x=\"room_type\",\n            y=\"log_price\",\n            data=df_10, \n            palette=\"tab10\", \n            order = list(sorted_hood_by_price.index)\n           ).set_title(\"Log(Price) by Room Type\", size=15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nsorted_hood_by_activity = df_10.groupby(['neighbourhood_group_cleansed'])['log_price'].median().sort_values()\nsns.violinplot(x=df_10.neighbourhood_group_cleansed, y=df_10.log_price, order=list(sorted_hood_by_activity.index))\nsns.despine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\n\nsorted_hood_by_activity = df_10.groupby(['room_type'])['log_reviews_pm'].median().sort_values()\nsns.violinplot(x=df_10.room_type, y=df_10.log_reviews_pm, order=list(sorted_hood_by_activity.index))\nsns.despine\n#private rooms getting reviewed the most","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scatterplot\nWeighted Scatterplots allow us to see relationships between many variables at once."},{"metadata":{"trusted":true},"cell_type":"code","source":"# UGLY, but potentially helpful plot\n\n# why may we want to do a scatterplot for categorical data??\n\n\nf, axes = plt.subplots(1, 1, figsize=(10, 15))\nsns.despine\n\nsns.scatterplot(x='log_guests_included', \n                y = 'log_price', \n                hue='room_type',\n                size = 'log_reviews_pm', \n                sizes= (5, 400),\n                alpha = .8,\n                palette = \"tab10\",\n                data = df_10).set_title(\"Price by Bedrooms (weighted by Reviews)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#appears that reviews_per_month is an important activity heuristic, \n#we can see there is a potential relationship between price and bedrooms, especially in active airbnbs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_10.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_10.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing columns that were effected by the log transformations."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Log Transformed Variables\n\ndf_lg = df_10.drop(['price', \n                    'security_deposit', \n                    'cleaning_fee', \n                    'extra_people', \n                    'reviews_per_month', \n                    'bedrooms', \n                    'bathrooms', \n                    'guests_included', \n                    'calculated_host_listings_count'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lg.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lg2 = df_lg.drop(['host_id', 'neighbourhood_cleansed', 'city', 'property_type'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lg2.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lg2.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lg2.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#MODELING STARTS\nX = df_lg2\nX = pd.get_dummies(data=X, drop_first = True)\n\nY = X['log_price']\nX2 = X.drop('log_price', axis = 1)\nX = X2\n\nX.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"                          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save cleaned X and Y for further analysis in R or elsewhere\n\nX.to_csv(\"./Xcleaned.csv\")\nY.to_csv(\"./Ycleaned.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BASELINE MULTIVARIATE REGRESSION MODEL\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .2, random_state = 40)\n\nest = sm.OLS(Y_train, X_train).fit()\ndisplay(est.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# explore Variance inflation factor of each variable in regression model\n\n# big VIF == bad\n\n# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n\nvif[\"features\"] = X_train.columns\n\nvif.round(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\n#EVALUATION OF MODEL\n\npredicted = est.predict(X_test)\n\nprint(\"MSE of model when comparing Y_test and predicted is %lf\" %mean_squared_error(Y_test, predicted))\n\n    \nfig, ax = plt.subplots(figsize=(7,7))\nax.scatter(Y_test, predicted)\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4, alpha=.5)\nax.set_xlabel('measured')\nax.set_ylabel('predicted')\nax.set_title(\"Baseline Train Predictions\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(5)\nplot_lm_1.set_figwidth(14)\n\nplot_lm_1.axes[0] = sns.residplot(est.fittedvalues, y=Y_train, data=X_train, \n                          lowess=True, \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_fitted_y = est.fittedvalues\n# model residuals\nmodel_residuals = est.resid\n# normalized residuals\nmodel_norm_residuals = est.get_influence().resid_studentized_internal\n# absolute squared normalized residuals\nmodel_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n# absolute residuals\nmodel_abs_resid = np.abs(model_residuals)\n# leverage, from statsmodels internals\nmodel_leverage = est.get_influence().hat_matrix_diag\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QQ = sm.ProbPlot(model_norm_residuals)\nplot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n\nplot_lm_2.set_figheight(6)\nplot_lm_2.set_figwidth(6)\n\nplot_lm_2.axes[0].set_title('Normal Q-Q')\nplot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\nplot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n\n# annotations\nabs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\nabs_norm_resid_top_3 = abs_norm_resid[:3]\n\nfor r, i in enumerate(abs_norm_resid_top_3):\n    plot_lm_2.axes[0].annotate(i, \n                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                   model_norm_residuals[i]));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(est.params, index = X_train.columns)\n\n\nimp_coef = pd.concat([coef.sort_values().head(18),\n                     coef.sort_values().tail(18)])\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Baseline Model\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}